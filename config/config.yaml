# Database Configuration
database:
  # PostgreSQL is the default database for production readiness
  url: "${DATABASE_URL:postgresql://kyna:kyna@localhost:5432/kyna}"

# Qdrant Vector Store Configuration
qdrant:
  host: "${QDRANT_HOST:localhost}"
  port: ${QDRANT_PORT:6333}
  collection_name: "kyna_faq"

# Embedding Model Configuration
embedding:
  # provider can be 'openai', 'fastembed', or 'sentence_transformers'
  provider: "fastembed"
  # The model name corresponding to the selected provider.
  # FastEmbed models: "BAAI/bge-small-en-v1.5", "BAAI/bge-base-en-v1.5", "BAAI/bge-large-en-v1.5"
  # Sentence Transformers models: "all-MiniLM-L6-v2", "all-mpnet-base-v2", "paraphrase-multilingual-MiniLM-L12-v2"
  # OpenAI models: "text-embedding-3-small", "text-embedding-3-large"
  model: "BAAI/bge-small-en-v1.5"

# LLM Provider Configuration (using LiteLLM)
llm:
  # The model string that LiteLLM will use to call the correct provider.
  # Examples: "gpt-3.5-turbo", "openrouter/google/gemini-pro", "groq/llama3-70b-8192"
  model: "gpt-3.5-turbo"
  # API keys are loaded automatically by LiteLLM from environment variables (e.g., OPENAI_API_KEY, OPENROUTER_API_KEY).

# Data Ingestion Configuration (Handled by API logic)
ingestion:
  chunk_size: 1200
  chunk_overlap: 200

# RAG Chain Configuration
rag:
  retriever:
    # search_type can be 'similarity' or 'similarity_score_threshold'
    search_type: "similarity"
    search_k: 10
    score_threshold: 0.2
  # Prompt template - can be file path or inline template
  prompt_template: "config/prompts/prompt.md"

# Conversational Memory Configuration
memory:
  # Time-to-live in seconds. Session data will be purged after this period of inactivity.
  ttl_seconds: 3600
  # Maximum number of messages to keep in history (e.g., 5 user + 5 AI = 10).
  # This prevents the memory buffer from growing indefinitely.
  max_history_length: 10